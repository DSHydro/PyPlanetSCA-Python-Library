{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b2f3fdb-f3a3-4e3d-a590-b2977553a474",
   "metadata": {},
   "source": [
    "# Welcome to PyPlanetSCA Demo using a Jupyter Notebook\n",
    "## This demo is the package restructured to run in a Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3be9eb8-f851-49b1-b497-1f85b6cda72e",
   "metadata": {},
   "source": [
    "## Start of the data gathering module\n",
    "### This cell contains all necessary imports as well as allowing the user to switch the apiKey and output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f342da75-2428-43da-a512-089fd264762b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import geopandas as gpd\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from shapely.geometry import shape\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import json\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "############# SET THESE VARIABLES ######################\n",
    "# redefined based on the filter you want to use\n",
    "# from geojson_GIN import domain\n",
    "# from geojson_STR import domain\n",
    "# from geojson_DPO import domain\n",
    "# from geojson_TUM import domain\n",
    "# from geojson_869 import domain\n",
    "# from geojson_DAN import domain\n",
    "# from geojson_793 import domain\n",
    "# from geojson_586 import domain\n",
    "# from geojson_737 import domain\n",
    "from geojson_551 import domain\n",
    "\n",
    "domainID = '551'\n",
    "\n",
    "# enter the Planet user API key - SWITCH\n",
    "apiKey = ''\n",
    "item_type = \"PSScene\"\n",
    "asset_type = \"ortho_analytic_4b_sr\"\n",
    "bundle_type = \"analytic_sr_udm2\"\n",
    "\n",
    "# data download location - SWITCH\n",
    "out_direc = ''\n",
    "############# DON'T CHANGE VARIABLES BEYOND THIS ############"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a6ed5a-1e3c-4a04-8e52-787ce20cc211",
   "metadata": {},
   "source": [
    "### Defines Functions for the data gathering module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b6e5f3-6de7-43dd-8347-f1b83fd3765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ FUNCTIONS. DON'T CHANGE THESE!!! #########\n",
    "def build_payload(item_ids, item_type, bundle_type, aoi_coordinates):\n",
    "    payload = {\n",
    "        \"name\": item_ids[0],\n",
    "        \"source_type\": \"scenes\",\n",
    "        \"products\": [\n",
    "            {\n",
    "                \"item_ids\": item_ids,\n",
    "                \"item_type\": item_type,\n",
    "                \"product_bundle\": bundle_type\n",
    "            }\n",
    "        ],\n",
    "        \"tools\": [\n",
    "            {\n",
    "                \"clip\": {\n",
    "                    \"aoi\": {\n",
    "                        \"type\": \"Polygon\",\n",
    "                        \"coordinates\": aoi_coordinates\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    return payload\n",
    "\n",
    "def order_now(payload,apiKey):\n",
    "    orders_url = 'https://api.planet.com/compute/ops/orders/v2'\n",
    "    response = requests.post(orders_url, data=json.dumps(payload), auth=HTTPBasicAuth(apiKey, ''), headers=headers)\n",
    "    print(response)\n",
    "\n",
    "    if response.status_code==202:\n",
    "        order_id =response.json()['id']\n",
    "        url = f\"https://api.planet.com/compute/ops/orders/v2/{order_id}\"\n",
    "        # feature_check = requests.get(url, auth=(PLANET_API_KEY, \"\"))\n",
    "        feature_check = requests.get(url, auth=HTTPBasicAuth(apiKey, ''))\n",
    "        if feature_check.status_code==200:\n",
    "            print(f\"Submitted a total of {len(feature_check.json()['products'][0]['item_ids'])} image ids: accepted a total of {len(feature_check.json()['products'][0]['item_ids'])} ids\")\n",
    "            print(f\"Order URL: https://api.planet.com/compute/ops/orders/v2/{order_id}\")\n",
    "            return f\"https://api.planet.com/compute/ops/orders/v2/{order_id}\"\n",
    "    else:\n",
    "        print(f'Failed with Exception code : {response.status_code}')\n",
    "        \n",
    "def download_results(order_url,folder, overwrite=False):\n",
    "    r = requests.get(order_url, auth=HTTPBasicAuth(apiKey, ''))\n",
    "    try:\n",
    "        if r.status_code ==200:\n",
    "            response = r.json()\n",
    "            results = response['_links']['results']\n",
    "            results_urls = [r['location'] for r in results]\n",
    "            results_names = [r['name'] for r in results]\n",
    "            print('{} items to download'.format(len(results_urls)))\n",
    "\n",
    "            for url, name in zip(results_urls, results_names):\n",
    "                path = pathlib.Path(os.path.join(folder,name))\n",
    "\n",
    "                if overwrite or not path.exists():\n",
    "                    print('downloading {} to {}'.format(name, path))\n",
    "                    r = requests.get(url, allow_redirects=True)\n",
    "                    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    open(path, 'wb').write(r.content)\n",
    "                else:\n",
    "                    print('{} already exists, skipping {}'.format(path, name))\n",
    "        else:\n",
    "            print(f'Failed with response {r.status_code}')\n",
    "    except:\n",
    "        print('data not ready yet')\n",
    "    r.close()\n",
    "    # except Exception as e:\n",
    "    #     print(e)\n",
    "    #     print(order_url)\n",
    "    #     raise Exception\n",
    "    # r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8f32e1-4bae-420e-bab7-a162e0f87b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ensure that the domain shape makes sense\n",
    "domain_geometry = shape(domain['config'][0]['config'])\n",
    "domain_geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a141a95a-9e09-4b00-bb2c-b6b9ca53b3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search API request object\n",
    "search_endpoint_request = {\n",
    "  \"item_types\": [item_type],\n",
    "  \"filter\": domain\n",
    "}\n",
    "result = \\\n",
    "  requests.post(\n",
    "    'https://api.planet.com/data/v1/quick-search',\n",
    "    auth=HTTPBasicAuth(apiKey, ''),\n",
    "    json=search_endpoint_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881eea00-f421-4000-bebb-bcd984760a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View available data and prepare the list of planet IDs to download\n",
    "geojson_data = result.json()\n",
    "gdf = gpd.GeoDataFrame.from_features(geojson_data['features'])\n",
    "\n",
    "# Add a new column to 'gdf' with the intersection area\n",
    "gdf['intersection_area'] = gdf['geometry'].intersection(domain_geometry).area\n",
    "\n",
    "# Calculate the percentage overlap\n",
    "gdf['overlap_percentage'] = (gdf['intersection_area'] / domain_geometry.area) * 100\n",
    "\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033c3ece-aeec-4c47-aded-e89e596902e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = [feature['id'] for idx, feature in enumerate(geojson_data['features']) if gdf['overlap_percentage'].iloc[idx] >= 99]\n",
    "geom_list = [feature['geometry'] for idx, feature in enumerate(geojson_data['features']) if gdf['overlap_percentage'].iloc[idx] >= 99]\n",
    "print(len(id_list))\n",
    "print(sorted(id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee28246-12b7-45af-b7da-0a6998959f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ONLY RUN THIS CELL IF YOU WANT TO CHECK ON ORDER STATUS\n",
    "\n",
    "# # see the status of the requested tiles. Are they \"active\"?\n",
    "# for IDD in id_list:\n",
    "#     print(IDD)\n",
    "#     command = 'curl -L -H \"Authorization: api-key '+apiKey+'\"'\n",
    "#     sublink = \" 'https://api.planet.com/data/v1/item-types/\"+item_type+\"/items/\"+IDD+\"/assets/' \"\n",
    "#     # sublink = \" 'https://api.planet.com/data/v2/item-types/\"+item_type+\"/items/\"+IDD+\"/assets/' \"\n",
    "#     command = command+sublink+'| jq .'+asset_type+'.status'\n",
    "#     status = subprocess.run(command, shell=True)\n",
    "#     print(command)\n",
    "#     # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44a9cad-daf4-4f41-8916-03f747628e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare and submit the orders\n",
    "order_urls = pd.DataFrame(columns = [\"index\",\"ID_geom\", \"order_url\"])\n",
    "\n",
    "# loop through each order payload, and submit\n",
    "for idx,IDD in enumerate(id_list):\n",
    "    print(idx,IDD)\n",
    "    \n",
    "    payload = build_payload([IDD],item_type,bundle_type,domain['config'][0]['config']['coordinates'])\n",
    "    order_url = order_now(payload,apiKey)\n",
    "    \n",
    "    order_urls.loc[idx, \"index\"] = idx        \n",
    "    order_urls.loc[idx, \"ID_geom\"] = IDD\n",
    "    order_urls.loc[idx, \"order_url\"] = order_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d5d86d-d169-4aa7-bd68-8d05a416b0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out the data, save to a csv if you want to come back later\n",
    "print(order_urls)\n",
    "order_urls.to_csv('urlSaver.csv', index = None)# save all URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4616bbf5-e9f4-4d47-bc2f-24a9f63671a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the orders once ready\n",
    "# outputs of \"data not ready yet\" mean that the orders need more time to process before downloading\n",
    "for url in order_urls.itertuples():\n",
    "    print(url.index,url.order_url)\n",
    "    print(\"start downloading data to\".format(), out_direc + url.ID_geom)\n",
    "    if url.order_url != None:\n",
    "        try:\n",
    "            nantest = ~np.isnan(url.order_url)\n",
    "        except:\n",
    "            download_results(url.order_url,folder = out_direc + url.ID_geom)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29c673a-306d-4261-9dfd-1916c2c91517",
   "metadata": {},
   "source": [
    "### Displays fp image to ensure that the data download has worked as necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca83e36-198a-4bca-ae75-3760205c8d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "#Replace with fp location\n",
    "fp = ''\n",
    "img = rasterio.open(fp)\n",
    "show(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ef763a-7291-445d-a0e9-2ab9cc49dbb7",
   "metadata": {},
   "source": [
    "## Data prepration module\n",
    "### Imports and defines SCA_Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36770265-b78f-43ee-a95f-27cb73819abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef69ec7d-e87b-4f60-9f97-a596d29c5441",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio import features\n",
    "from rasterio.mask import mask\n",
    "from rasterio.enums import MergeAlg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import joblib\n",
    "import time\n",
    "import os\n",
    "import string\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import json\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# rasterize the ROI for model training - USED\n",
    "def vector_rasterize(dir_vector, dir_raster, dir_out, flag_output):\n",
    "    vector = gpd.read_file(dir_vector)\n",
    "    # Get list of geometries for all features in vector file\n",
    "    geom = [shapes for shapes in vector.geometry]\n",
    "\n",
    "    # Open example raster\n",
    "    raster = rasterio.open(dir_raster)\n",
    "    \n",
    "    # reproject vector to raster\n",
    "    vector = vector.to_crs(raster.crs)\n",
    "\n",
    "    # create tuples of geometry, value pairs, where value is the attribute value you want to burn\n",
    "    geom_value = ((geom,value) for geom, value in zip(vector.geometry, vector['label']))\n",
    "\n",
    "    # Rasterize vector using the shape and transform of the raster\n",
    "    rasterized = features.rasterize(geom_value,\n",
    "                                    out_shape = raster.shape,\n",
    "                                    transform = raster.transform,\n",
    "                                    all_touched = True,\n",
    "                                    fill = 9,   # background value\n",
    "                                    merge_alg = MergeAlg.replace,\n",
    "                                    dtype = np.float32)\n",
    "\n",
    "\n",
    "    if flag_output:\n",
    "        with rasterio.open(\n",
    "                dir_out, \"w\",\n",
    "                driver = \"GTiff\",\n",
    "                transform = raster.transform,\n",
    "                dtype = rasterio.float32,\n",
    "                count = 1,\n",
    "                width = raster.width,\n",
    "                height = raster.height) as dst:\n",
    "            dst.write(rasterized, indexes = 1)\n",
    "    return rasterized\n",
    "\n",
    "def run_sca_prediction(dir_raster, dir_out, nodata_flag, model):\n",
    "    \"\"\"\n",
    "    This function predicts binary snow cover for planet satellite images using \n",
    "    the pre-trained random forest model \n",
    "    \n",
    "    :param dir_raster: the directory or the file of planet images\n",
    "    :param dir_out: the directory where output snow cover images will be stored\n",
    "    :param nodata_flag: the value used to represent no data in the predicted snow cover image\n",
    "    defult value is 9.\n",
    "    model: the model used to predict snow cover\n",
    "    \n",
    "    \"\"\"\n",
    "    # if output directory not exist then creat the output directory\n",
    "    if not os.path.exists(dir_out): os.mkdir(dir_out)\n",
    "    \n",
    "    # if dir_raster is a directory, then find all images with 'SR' flag, meaning surface reflectance data\n",
    "    if os.path.isdir(dir_raster):\n",
    "        file_list = glob.glob(dir_raster + './**/*SR*.tif', recursive = True)\n",
    "    elif os.path.isfile(dir_raster):\n",
    "        file_list = [dir_raster]\n",
    "\n",
    "    print(file_list)\n",
    "    for f in file_list:\n",
    "        print('Start to predict:'.format(), os.path.basename(f))\n",
    "\n",
    "        with rasterio.open(f, 'r') as ds:\n",
    "            arr = ds.read()  # read all raster values\n",
    "            if arr.shape[0] > 4: # if we have more than 4 bands\n",
    "                arr = arr[:4,:,:] # use only the first four\n",
    "\n",
    "        print(\"Image dimension:\".format(), arr.shape)  # \n",
    "        X_img = pd.DataFrame(arr.reshape([4,-1]).T)\n",
    "        X_img.columns = ['blue','green','red','nir']\n",
    "        \n",
    "        X_img = X_img/10000 # scale surface reflectance to 0-1\n",
    "        \n",
    "        X_img['nodata_flag'] = np.where(X_img['blue']==0, -1, 1) # wherever blue band is zero, set to nodata value of -1\n",
    "        \n",
    "        \n",
    "        # run model prediction\n",
    "        y_img = model.predict(X_img.iloc[:,0:4])\n",
    "        \n",
    "        out_img = pd.DataFrame()\n",
    "        out_img['label'] = y_img\n",
    "        out_img['nodata_flag'] = X_img['nodata_flag']\n",
    "        out_img['label'] = np.where(out_img['nodata_flag'] == -1, nodata_flag, out_img['label']) # where we set to -1, set to new nodata_flag value\n",
    "        # Reshape our classification map\n",
    "        img_prediction = out_img['label'].to_numpy().reshape(arr[0,:, :].shape)\n",
    "\n",
    "        \n",
    "        file_out = dir_out + os.path.basename(f)[0:-4] + '_SCA.tif'\n",
    "        print(\"Save SCA map to: \".format(),file_out)\n",
    "        with rasterio.open(\n",
    "                        file_out, \"w\",\n",
    "                        driver = \"GTiff\",\n",
    "                        transform = ds.transform,\n",
    "                        dtype = rasterio.uint8,\n",
    "                        count = 1,\n",
    "                        crs = ds.crs,\n",
    "                        width = ds.width,\n",
    "                        height = ds.height,\n",
    "                        nodata = nodata_flag) as dst:\n",
    "                    dst.write(img_prediction, indexes = 1, masked=True)\n",
    "                            \n",
    "\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3786b1-5891-4b11-a57a-8b5ce79008d0",
   "metadata": {},
   "source": [
    "### Creates training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b8c7ed-923c-4b75-a638-513377280985",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_rasterize = False\n",
    "\n",
    "# dir_ROI = \"./data/ROI/20180528_181110_add_UTM11.shp\"\n",
    "# dir_raster = \"./data/planet/train/20180528_181110_1025_3B_AnalyticMS_SR_clip.tif\"\n",
    "# dir_ROIraster = './data/ROI/ROI_20180528_181110_add.tif'\n",
    "# dir_samples_root = './data/samples/sample_'\n",
    "# dir_samples = 'sample_174k.csv'\n",
    "\n",
    "dir_ROI = \"\"\n",
    "dir_raster = \"\"\n",
    "dir_ROIraster = ''\n",
    "dir_samples_root = ''\n",
    "dir_samples = 'sample_174k.csv'\n",
    "\n",
    "if flag_rasterize:\n",
    "    flag_output = True\n",
    "    # rasterize ROI\n",
    "    ROI = vector_rasterize(dir_vector=dir_ROI, dir_raster=dir_raster, dir_out=dir_ROIraster, flag_output = flag_output)\n",
    "    \n",
    "    # save surface reflectance and lable to csv file\n",
    "    N_scale = 10000.0\n",
    "    img = rasterio.open(dir_raster)\n",
    "    ROI = rasterio.open(dir_ROIraster)\n",
    "    img_read = img.read()/N_scale\n",
    "    df_img = pd.DataFrame(img_read.reshape([4,-1]).T)\n",
    "    df_label = pd.DataFrame(ROI.read().reshape([1,-1]).T)\n",
    "\n",
    "    df_train = pd.concat([df_img, df_label], axis = 1)\n",
    "    df_train.columns = ['blue','green','red','nir','label']\n",
    "    df_train['ndvi'] = (df_train['nir']-df_train['red'])/(df_train['nir']+df_train['red'])\n",
    "    df_train = df_train[df_train.label !=9]\n",
    "    df_train.label = np.where(df_train.label > 0, 1, 0)\n",
    "    dir_samples = dir_samples_root + str(int(len(df_train.index)/1000)) + 'k.csv'\n",
    "    df_train.to_csv(dir_samples, index = False)\n",
    "else:\n",
    "    df_train = pd.read_csv(dir_samples)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1749af7-e473-49d9-abff-ba55272ca94a",
   "metadata": {},
   "source": [
    "## Model Training Module\n",
    "### Prints model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e01944-52bd-4b77-810a-ca20e373fa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_train = False\n",
    "\n",
    "# get data \n",
    "dir_model = 'random_forest_20240116_binary_174K.joblib'\n",
    "dir_score = 'random_forest_20240116_binary_174K_scores.csv'\n",
    "starttime = time.process_time()\n",
    "if True:\n",
    "    X = df_train[['blue', 'green','red','nir']]\n",
    "    y = df_train['label']\n",
    "    \n",
    "    # pre-process ndvi value to -1.0 to 1.0; fill nan to finite value \n",
    "    # X[X['ndvi']< -1.0]['ndvi'] = -1.0\n",
    "    # X[X['ndvi']> 1.0]['ndvi'] = 1.0\n",
    "    # X[np.isfinite(X['ndvi']) == False]['ndvi'] = np.nan\n",
    "\n",
    "    # define the model\n",
    "    model = RandomForestClassifier(n_estimators=10, max_depth=10, max_features=4, random_state=1)#\n",
    "    # evaluate the model\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=1)\n",
    "    n_accuracy = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    n_f1 = cross_val_score(model, X, y, scoring='f1', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    n_balanced_accuracy = cross_val_score(model, X, y, scoring='balanced_accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    \n",
    "    # report performance\n",
    "    plt.hist(n_f1)\n",
    "    print('Repeat times:'.format(), len(n_f1))\n",
    "    print('F1-score: %.5f (%.5f)' % (n_f1.mean(), n_f1.std()))\n",
    "    print('Balanced Accuracy: %.5f (%.5f)' % (n_balanced_accuracy.mean(), n_balanced_accuracy.std()))\n",
    "    print('Accuracy: %.5f (%.5f)' % (n_accuracy.mean(), n_accuracy.std()))\n",
    "\n",
    "    # fit model with all observations\n",
    "    model.fit(X,y)\n",
    "    # save model \n",
    "    joblib.dump(model, dir_model)\n",
    "    # save accuracy \n",
    "    scores = pd.DataFrame()\n",
    "    scores[\"accuracy\"] = n_accuracy\n",
    "    scores['f1'] = n_f1\n",
    "    scores['balanced_accuracy'] = n_balanced_accuracy\n",
    "    scores.to_csv(dir_score, index = False)\n",
    "\n",
    "    print('Total time used:'.format(), round(time.process_time() - starttime, 1))\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938102c8-dcd6-4e65-b72a-43bfa9eb9600",
   "metadata": {},
   "source": [
    "## Prediction Evaluation module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4227902b-ece0-43dc-a147-71dddc92d920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run prediction for all images in a folder\n",
    "\n",
    "#REPLACE\n",
    "dir_raster = ''\n",
    "dir_model = \"\"\n",
    "dir_out = ''\n",
    "\n",
    "#Custom code to run for single image\n",
    "model = joblib.load(dir_model)\n",
    "nodata_flag = 9\n",
    "run_sca_prediction(dir_raster, dir_out, nodata_flag, model)\n",
    "            \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12992306-4235-4965-9435-7aefa70e2800",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REPLACE\n",
    "temp = rasterio.open(\"\", 'r').read(masked = True)\n",
    "plt.imshow(temp.squeeze())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
